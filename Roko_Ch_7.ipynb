{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1259efbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>VotingClassifier(estimators=[(&#x27;lr&#x27;, LogisticRegression(random_state=42)),\n",
       "                             (&#x27;rf&#x27;, RandomForestClassifier(random_state=42)),\n",
       "                             (&#x27;svc&#x27;, SVC(random_state=42))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">VotingClassifier</label><div class=\"sk-toggleable__content\"><pre>VotingClassifier(estimators=[(&#x27;lr&#x27;, LogisticRegression(random_state=42)),\n",
       "                             (&#x27;rf&#x27;, RandomForestClassifier(random_state=42)),\n",
       "                             (&#x27;svc&#x27;, SVC(random_state=42))])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>lr</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(random_state=42)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>rf</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(random_state=42)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>svc</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(random_state=42)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "VotingClassifier(estimators=[('lr', LogisticRegression(random_state=42)),\n",
       "                             ('rf', RandomForestClassifier(random_state=42)),\n",
       "                             ('svc', SVC(random_state=42))])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Chapter 7. Ensemble Learning and Random Forests\n",
    "#Suppose you pose a complex question to thousands of random people, then aggregate their answers. \n",
    "#In many cases you will find that this aggregated answer is better than an expert’s answer. This is\n",
    "#called the wisdom of the crowd. Similarly, if you aggregate the predictions of a group of predictors\n",
    "#(such as classifiers or regressors), you will often get better predictions than with the best\n",
    "#individual predictor. A group of predictors is called an ensemble; thus, this technique is \n",
    "#called ensemble learning, and an ensemble learning algorithm is called an ensemble method.\n",
    "\n",
    "#As an example of an ensemble method, you can train a group of decision tree classifiers, each \n",
    "#on a different random subset of the training set. You can then obtain the predictions of all \n",
    "#the individual trees, and the class that gets the most votes is the ensemble’s prediction \n",
    "#(see the last exercise in Chapter 6). Such an ensemble of decision trees is called a random forest,\n",
    "#and despite its simplicity, this is one of the most powerful machine learning algorithms available today.\n",
    "\n",
    "#As discussed in Chapter 2, you will often use ensemble methods near the end of a project, once \n",
    "#you have already built a few good predictors, to combine them into an even better predictor. \n",
    "\n",
    "#In fact, the winning solutions in machine learning competitions often involve several ensemble\n",
    "#methods—most famously in the Netflix Prize competition.\n",
    "#In this chapter we will examine the most popular ensemble methods, including voting classifiers, \n",
    "#bagging and pasting ensembles, random forests, and boosting, and stacking ensembles.\n",
    "\n",
    "#A very simple way to create an even better classifier is to aggregate the predictions of each \n",
    "#classifier: the class that gets the most votes is the ensemble’s prediction. This majority-vote\n",
    "#classifier is called a hard voting classifier (see Figure 7-2).\n",
    "\n",
    "\n",
    "#Ensemble methods work best when the predictors are as independent from one another as possible.\n",
    "#One way to get diverse classifiers is to train them using very different algorithms. This increases\n",
    "#the chance that they will make very different types of errors, improving the ensemble’s accuracy.\n",
    "\n",
    "\n",
    "#Scikit-Learn provides a VotingClassifier class that’s quite easy to use: just give it a list of \n",
    "#name/predictor pairs, and use it like a normal classifier. Let’s try it on the moons dataset \n",
    "#(introduced in Chapter 5). We will load and split the moons dataset into a training set and a \n",
    "#test set, then we’ll create and train a voting classifier composed of three diverse classifiers:\n",
    "\n",
    "\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "X, y = make_moons(n_samples=500, noise=0.30, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "       estimators=[\n",
    "           ('lr', LogisticRegression(random_state=42)),\n",
    "           ('rf', RandomForestClassifier(random_state=42)),\n",
    "           ('svc', SVC(random_state=42))\n",
    "])\n",
    "\n",
    "voting_clf.fit(X_train, y_train)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e46f8f78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lr', LogisticRegression(random_state=42)),\n",
       " ('rf', RandomForestClassifier(random_state=42)),\n",
       " ('svc', SVC(random_state=42))]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#When you fit a VotingClassifier, it clones every estimator and fits the clones. The original estimators\n",
    "#are available via the estimators attribute, while the fitted clones are available via the estimators_ \n",
    "#attribute. If you prefer a dict rather than a list, you can use named_estimators or named_estimators_\n",
    "#instead. To begin, let’s look at each fitted classifier’s accuracy on the test set:\n",
    "\n",
    "voting_clf.estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0212912",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LogisticRegression(random_state=42),\n",
       " RandomForestClassifier(random_state=42),\n",
       " SVC(random_state=42)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_clf.estimators_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc25d805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr = 0.864\n",
      "rf = 0.896\n",
      "svc = 0.896\n"
     ]
    }
   ],
   "source": [
    "for name, clf in voting_clf.named_estimators_.items():\n",
    "    print(name, \"=\", clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dc36c5fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lr': LogisticRegression(random_state=42),\n",
       " 'rf': RandomForestClassifier(random_state=42),\n",
       " 'svc': SVC(probability=True, random_state=42)}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_clf.named_estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d74c603",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#When you call the voting classifier’s predict() method, it performs hard voting. \n",
    "#For example, the voting classifier predicts class 1 for the first instance of the \n",
    "#test set, because two out of three classifiers predict that class:\n",
    "\n",
    "voting_clf.predict(X_test[:1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86b3751f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LogisticRegression(random_state=42),\n",
       " RandomForestClassifier(random_state=42),\n",
       " SVC(random_state=42)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_clf.estimators_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39092bf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([1]), array([1]), array([0])]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[clf.predict(X_test[:1]) for clf in voting_clf.estimators_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2a1085a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.912"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now let’s look at the performance of the voting classifier on the test set:\n",
    "\n",
    "voting_clf.score(X_test, y_test) #hard voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd91c997",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.92"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#There you have it! The voting classifier outperforms all the individual classifiers.\n",
    "\n",
    "\n",
    "#If all classifiers are able to estimate class probabilities (i.e., if they all have a predict_proba() method),\n",
    "#then you can tell Scikit-Learn to predict the class with the highest class probability, averaged over all the\n",
    "#individual classifiers. This is called soft voting. It often achieves higher performance than hard voting\n",
    "#because it gives more weight to highly confident votes. All you need to do is set the voting classifier’s\n",
    "#voting hyperparameter to \"soft\", and ensure that all classifiers can estimate class probabilities. This\n",
    "#is not the case for the SVC class by default, so you need to set its probability hyperparameter to True\n",
    "#(this will make the SVC class use cross-validation to estimate class probabilities, slowing down training,\n",
    "#and it will add a predict_proba() method). Let’s try that:\n",
    "\n",
    "\n",
    "voting_clf.voting = \"soft\" #soft voting\n",
    "voting_clf.named_estimators[\"svc\"].probability = True #set to original estimator, not the cloned one (with _ at the end)\n",
    "voting_clf.fit(X_train, y_train)\n",
    "voting_clf.score(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bca31195",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We reach 92% accuracy simply by using soft voting—not bad!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "57481a9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>BaggingClassifier(base_estimator=DecisionTreeClassifier(), max_samples=100,\n",
       "                  n_estimators=500, n_jobs=1, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">BaggingClassifier</label><div class=\"sk-toggleable__content\"><pre>BaggingClassifier(base_estimator=DecisionTreeClassifier(), max_samples=100,\n",
       "                  n_estimators=500, n_jobs=1, random_state=42)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">base_estimator: DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "BaggingClassifier(base_estimator=DecisionTreeClassifier(), max_samples=100,\n",
       "                  n_estimators=500, n_jobs=1, random_state=42)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Bagging and Pasting\n",
    "#One way to get a diverse set of classifiers is to use very different training algorithms, as just discussed. \n",
    "#Another approach is to use the same training algorithm for every predictor but train them on different random\n",
    "#subsets of the training set. When sampling is performed with replacement,1 this method is called bagging \n",
    "#(short for bootstrap aggregating3). When sampling is performed without replacement, it is called pasting.\n",
    "#In other words, both bagging and pasting allow training instances to be sampled several times across multiple\n",
    "#predictors, but only bagging allows training instances to be sampled several times for the same predictor.\n",
    "#This sampling and training process is represented in Figure 7-4.\n",
    "\n",
    "#Once all predictors are trained, the ensemble can make a prediction for a new instance by simply aggregating \n",
    "#the predictions of all predictors. The aggregation function is typically the statistical mode for classification\n",
    "#(i.e., the most frequent prediction, just like with a hard voting classifier), or the average for regression.\n",
    "#Each individual predictor has a higher bias than if it were trained on the original training set, but aggregation\n",
    "#reduces both bias and variance.5 Generally, the net result is that the ensemble has a similar bias but a lower\n",
    "#variance than a single predictor trained on the original training set.\n",
    "\n",
    "#As you can see in Figure 7-4, predictors can all be trained in parallel, via different CPU cores or even\n",
    "#different servers. Similarly, predictions can be made in parallel. This is one of the reasons bagging\n",
    "#and pasting are such popular methods: they scale very well.\n",
    "\n",
    "\n",
    "#Bagging and Pasting in Scikit-Learn\n",
    "#Scikit-Learn offers a simple API for both bagging and pasting: BaggingClassifier class (or BaggingRegressor\n",
    "#for regression). The following code trains an ensemble of 500 decision tree classifiers:6 each is trained\n",
    "#on 100 training instances randomly sampled from the training set with replacement (this is an example of\n",
    "#bagging, but if you want to use pasting instead, just set bootstrap=False). The n_jobs parameter tells \n",
    "#Scikit-Learn the number of CPU cores to use for training and predictions, and –1 tells Scikit-Learn to\n",
    "#use all available cores:\n",
    "    \n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "bag_clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators=500,\n",
    "                           max_samples=100, n_jobs=1, random_state=42)\n",
    "bag_clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "#A BaggingClassifier automatically performs soft voting instead of hard voting if the base classifier can \n",
    "#estimate class probabilities (i.e., if it has a predict_proba() method), which is the case with decision \n",
    "#tree classifiers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1c3599cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.896"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Figure 7-5 compares the decision boundary of a single decision tree with the decision boundary of a bagging \n",
    "#ensemble of 500 trees (from the preceding code), both trained on the moons dataset. As you can see, the \n",
    "#ensemble’s predictions will likely generalize much better than the single decision tree’s predictions: the\n",
    "#ensemble has a comparable bias but a smaller variance (it makes roughly the same number of errors on the \n",
    "#training set, but the decision boundary is less irregular).\n",
    "\n",
    "#Bagging introduces a bit more diversity in the subsets that each predictor is trained on, so bagging ends up \n",
    "#with a slightly higher bias than pasting; but the extra diversity also means that the predictors end up being \n",
    "#less correlated, so the ensemble’s variance is reduced. Overall, bagging often results in better models, which\n",
    "#explains why it’s generally preferred. But if you have spare time and CPU power, you can use cross- validation\n",
    "#to evaluate both bagging and pasting and select the one that works best.\n",
    "\n",
    "\n",
    "#Out-of-Bag Evaluation\n",
    "#With bagging, some training instances may be sampled several times for any given predictor, while others may\n",
    "#not be sampled at all. By default a BaggingClassifier samples m training instances with replacement \n",
    "#(bootstrap=True), where m is the size of the training set. With this process, it can be shown mathematically\n",
    "#that only about 63% of the training instances are sampled on average for each predictor.7 The remaining 37%\n",
    "#of the training instances that are not sampled are called out-of-bag (OOB) instances. Note that they are not\n",
    "#the same 37% for all predictors.\n",
    "\n",
    "\n",
    "#A bagging ensemble can be evaluated using OOB instances, without the need for a separate validation set: \n",
    "#indeed, if there are enough estimators, then each instance in the training set will likely be an OOB \n",
    "#instance of several estimators, so these estimators can be used to make a fair ensemble prediction for\n",
    "#that instance. Once you have a prediction for each instance, you can compute the ensemble’s prediction\n",
    "#accuracy (or any other metric).\n",
    "\n",
    "\n",
    "#In Scikit-Learn, you can set oob_score=True when creating a BaggingClassifier to request an automatic OOB \n",
    "#evaluation after training. The following code demonstrates this. The resulting evaluation score is \n",
    "#available in the oob_score_ attribute:\n",
    "\n",
    "\n",
    "bag_clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators=500,\n",
    "                           oob_score=True, n_jobs=-1, random_state=42)\n",
    "\n",
    "bag_clf.fit(X_train, y_train)\n",
    "bag_clf.oob_score_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "840126c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.912"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#According to this OOB evaluation, this BaggingClassifier is likely to achieve about 89.6% accuracy on the test set. Let’s verify this:\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = bag_clf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bf4dbf85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.912"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf.score(X_test, y_test) #quicker than accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3eb2a88d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.32352941, 0.67647059],\n",
       "       [0.3375    , 0.6625    ],\n",
       "       [1.        , 0.        ]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We get 91.2% accuracy on the test. The OOB evaluation was a bit too pessimistic, just over 1% too low.\n",
    "\n",
    "\n",
    "#The OOB decision function for each training instance is also available through the oob_decision_function_ attribute.\n",
    "#Since the base estimator has a predict_proba() method, the decision function returns the class probabilities\n",
    "#for each training instance. For example, the OOB evaluation estimates that the first training instance has\n",
    "#a 67.6% probability of belonging to the positive class and a 32.4% probability of belonging to the negative class:\n",
    "\n",
    "bag_clf.oob_decision_function_[:3]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "545a09ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forests\n",
    "#As we have discussed, a random forest10 is an ensemble of decision trees, generally trained via the bagging\n",
    "#method (or sometimes pasting), typically with max_samples set to the size of the training set. Instead of\n",
    "#building a BaggingClassifier and passing it a DecisionTreeClassifier, you can use the RandomForestClassifier\n",
    "#class, which is more convenient and optimized for decision trees11 (similarly, there is a RandomForestRegressor\n",
    "#class for regression tasks). The following code trains a random forest classifier with 500 trees, each limited\n",
    "#to maximum 16 leaf nodes, using all available CPU cores:\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, \n",
    "                                n_jobs=1, random_state=42)\n",
    "\n",
    "rnd_clf.fit(X_train, y_train)\n",
    "y_pred_rf = rnd_clf.predict(X_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "02b30551",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.912"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2839b82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#With a few exceptions, a RandomForestClassifier has all the hyperparameters of a DecisionTreeClassifier \n",
    "#(to control how trees are grown), plus all the hyperparameters of a BaggingClassifier to control the\n",
    "#ensemble itself.\n",
    "\n",
    "#The random forest algorithm introduces extra randomness when growing trees; instead\n",
    "#of searching for the very best feature when splitting a node (see Chapter 6), it searches for the best \n",
    "#feature among a random subset of features By default, it samples sqrt(n) features (where n is the total \n",
    "#number of features). The algorithm results in greater tree for the best feature among a random subset of features.\n",
    "#The algorithm results in greater tree diversity, which (again) trades a higher bias for a lower variance, \n",
    "#generally yielding an overall better model. So, the following BaggingClassifier is equivalent to the\n",
    "#previous RandomForestClassifier:\n",
    "\n",
    "\n",
    "bag_clf = BaggingClassifier(DecisionTreeClassifier(max_features=\"sqrt\", \n",
    "                                                  max_leaf_nodes=16),\n",
    "                            n_estimators=500, n_jobs=-1, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1b6b7853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11 sepal length (cm)\n",
      "0.02 sepal width (cm)\n",
      "0.44 petal length (cm)\n",
      "0.42 petal width (cm)\n"
     ]
    }
   ],
   "source": [
    "#Extra-Trees\n",
    "#When you are growing a tree in a random forest, at each node only a random subset of\n",
    "#the features is considered for splitting (as discussed earlier). It is possible to make trees\n",
    "#even more random by also using random thresholds for each feature rather than\n",
    "#searching for the best possible thresholds (like regular decision trees do). For this,\n",
    "#simply set splitter=\"random\" when creating a DecisionTreeClassifier.\n",
    "#A forest of such extremely random trees is called an extremely randomized trees12 (or\n",
    "#extra-trees for short) ensemble. Once again, this technique trades more bias for a lower\n",
    "#variance. It also makes extra-trees classifiers much faster to train than regular random\n",
    "#forests, because finding the best possible threshold for each feature at every node is one\n",
    "#of the most time-consuming tasks of growing a tree.\n",
    "#You can create an extra-trees classifier using Scikit-Learn’s ExtraTreesClassifier\n",
    "#class. Its API is identical to the RandomForestClassifier class, except bootstrap\n",
    "#defaults to False. Similarly, the ExtraTreesRegressor class has the same API as the\n",
    "#RandomForestRegressor class, except bootstrap defaults to False.\n",
    "\n",
    "#It is hard to tell in advance whether a RandomForestClassifier will perform better or worse than an \n",
    "#ExtraTreesClassifier. Generally, the only way to know is to try both and compare them using cross-validation.\n",
    "\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris(as_frame=True)\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500, random_state=42)\n",
    "rnd_clf.fit(iris.data, iris.target)\n",
    "for score, name in zip(rnd_clf.feature_importances_, iris.data.columns):\n",
    "    print(round(score, 2), name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "10b20141",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
       "0                5.1               3.5                1.4               0.2\n",
       "1                4.9               3.0                1.4               0.2\n",
       "2                4.7               3.2                1.3               0.2"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "33ced9da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    0\n",
       "2    0\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.target.head(3\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "26f4c72c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;background-color: white;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1),\n",
       "                   learning_rate=0.5, n_estimators=30, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" ><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">AdaBoostClassifier</label><div class=\"sk-toggleable__content\"><pre>AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1),\n",
       "                   learning_rate=0.5, n_estimators=30, random_state=42)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" ><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">base_estimator: DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(max_depth=1)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-12\" type=\"checkbox\" ><label for=\"sk-estimator-id-12\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(max_depth=1)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1),\n",
       "                   learning_rate=0.5, n_estimators=30, random_state=42)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Random forests are very handy to get a quick understanding of what features actually matter, in particular\n",
    "#if you need to perform feature selection.\n",
    "\n",
    "\n",
    "#Boosting\n",
    "#Boosting (originally called hypothesis boosting) refers to any ensemble method that can combine several \n",
    "#weak learners into a strong learner. The general idea of most boosting methods is to train predictors \n",
    "#sequentially, each trying to correct its predecessor. There are many boosting methods available, but\n",
    "#by far the most popular are AdaBoost13 (short for adaptive boosting) and gradient boosting. Let’s \n",
    "#start with AdaBoost.\n",
    "\n",
    "#AdaBoost\n",
    "#One way for a new predictor to correct its predecessor is to pay a bit more attention to the training \n",
    "#instances that the predecessor underfit. This results in new predictors focusing more and more on the \n",
    "#hard cases. This is the technique used by AdaBoost.\n",
    "\n",
    "#For example, when training an AdaBoost classifier, the algorithm first trains a base classifier\n",
    "#(such as a decision tree) and uses it to make predictions on the training set. The algorithm then \n",
    "#increases the relative weight of misclassified training instances. Then it trains a second classifier,\n",
    "#using the updated weights, and again makes predictions on the training set, updates the instance weights,\n",
    "#and so on (see Figure 7-7).\n",
    "#Figure 7-8 shows the decision boundaries of five consecutive predictors on the moons dataset\n",
    "#(in this example, each predictor is a highly regularized SVM classifier with an RBF kernel).14 \n",
    "#The first classifier gets many instances wrong, so their weights get boosted. The second classifier \n",
    "#therefore does a better job on these instances, and so on. The plot on the right represents the same\n",
    "#sequence of predictors, except that the learning rate is halved (i.e., the misclassified instance \n",
    "#weights are boosted much less at every iteration). As you can see, this sequential learning technique \n",
    "#has some similarities with gradient descent, except that instead of tweaking a single predictor’s \n",
    "#parameters to minimize a cost function, AdaBoost adds predictors to the ensemble, gradually making it better.\n",
    "\n",
    "#Once all predictors are trained, the ensemble makes predictions very much like bagging or pasting, \n",
    "#except that predictors have different weights depending on their overall accuracy on the weighted training set.\n",
    "\n",
    "\n",
    "#There is one important drawback to this sequential learning technique: training cannot be parallelized\n",
    "#since each predictor can only be trained after the previous predictor has been trained and evaluated.\n",
    "#As a result, it does not scale as well as bagging or pasting.\n",
    "\n",
    "#maths explained on page 290-291!!!!\n",
    "\n",
    "#The following code trains an AdaBoost classifier based on 30 decision stumps using\n",
    "#Scikit-Learn’s AdaBoostClassifier class (as you might expect, there is also an\n",
    "#AdaBoostRegressor class). A decision stump is a decision tree with max_depth=1—in\n",
    "#other words, a tree composed of a single decision node plus two leaf nodes. This is the\n",
    "#default base estimator for the AdaBoostClassifier class:\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "\n",
    "ada_clf = AdaBoostClassifier(\n",
    "        DecisionTreeClassifier(max_depth=1), n_estimators=30,\n",
    "        learning_rate=0.5, random_state=42)\n",
    "\n",
    "ada_clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "#If your AdaBoost ensemble is overfitting the training set, you can try reducing the number of estimators \n",
    "#or more strongly regularizing the base estimator.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2e27ebef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-6 {color: black;background-color: white;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeRegressor(max_depth=1, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-13\" type=\"checkbox\" checked><label for=\"sk-estimator-id-13\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeRegressor</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeRegressor(max_depth=1, random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeRegressor(max_depth=1, random_state=42)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Gradient Boosting\n",
    "\n",
    "\n",
    "#Another very popular boosting algorithm is gradient boosting.17 Just like AdaBoost,\n",
    "#gradient boosting works by sequentially adding predictors to an ensemble, each one\n",
    "#correcting its predecessor. However, instead of tweaking the instance weights at every\n",
    "#iteration like AdaBoost does, this method tries to fit the new predictor to the residual\n",
    "#errors made by the previous predictor.\n",
    "#Let’s go through a simple regression example, using decision trees as the base\n",
    "#predictors; this is called gradient tree boosting, or gradient boosted regression trees\n",
    "#(GBRT). First, let’s generate a noisy quadratic dataset and fit a\n",
    "#DecisionTreeRegressor to it:\n",
    "    \n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 1) - 0.5\n",
    "y = 3 * X[:, 0] ** 2 + 0.05 * np.random.randn(100) # y= 3x**2 + Gaussian noise\n",
    "\n",
    "tree_reg1 = DecisionTreeRegressor(max_depth=1, random_state=42)\n",
    "tree_reg1.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "10103672",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-7 {color: black;background-color: white;}#sk-container-id-7 pre{padding: 0;}#sk-container-id-7 div.sk-toggleable {background-color: white;}#sk-container-id-7 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-7 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-7 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-7 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-7 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-7 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-7 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-7 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-7 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-7 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-7 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-7 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-7 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-7 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-7 div.sk-item {position: relative;z-index: 1;}#sk-container-id-7 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-7 div.sk-item::before, #sk-container-id-7 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-7 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-7 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-7 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-7 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-7 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-7 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-7 div.sk-label-container {text-align: center;}#sk-container-id-7 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-7 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-7\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeRegressor(max_depth=2, random_state=43)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-14\" type=\"checkbox\" checked><label for=\"sk-estimator-id-14\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeRegressor</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeRegressor(max_depth=2, random_state=43)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeRegressor(max_depth=2, random_state=43)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Next, we’ll train a second DecisionTreeRegressor on the residual errors made by the first predictor:\n",
    "\n",
    "y2 = y - tree_reg1.predict(X)\n",
    "tree_reg2 = DecisionTreeRegressor(max_depth=2, random_state=43)\n",
    "tree_reg2.fit(X, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1f3ee17b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-8 {color: black;background-color: white;}#sk-container-id-8 pre{padding: 0;}#sk-container-id-8 div.sk-toggleable {background-color: white;}#sk-container-id-8 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-8 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-8 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-8 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-8 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-8 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-8 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-8 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-8 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-8 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-8 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-8 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-8 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-8 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-8 div.sk-item {position: relative;z-index: 1;}#sk-container-id-8 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-8 div.sk-item::before, #sk-container-id-8 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-8 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-8 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-8 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-8 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-8 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-8 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-8 div.sk-label-container {text-align: center;}#sk-container-id-8 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-8 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-8\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeRegressor(max_depth=2, random_state=44)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-15\" type=\"checkbox\" checked><label for=\"sk-estimator-id-15\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeRegressor</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeRegressor(max_depth=2, random_state=44)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeRegressor(max_depth=2, random_state=44)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#And then we’ll train a third regressor on the residual errors made by the second predictor:\n",
    "\n",
    "y3 = y2 - tree_reg2.predict(X)\n",
    "tree_reg3 = DecisionTreeRegressor(max_depth=2, random_state=44)\n",
    "tree_reg3.fit(X, y3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7b5fec05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.62493205, 0.01135118, 0.62505246])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now we have an ensemble containing three trees. It can make predictions on a new instance simply by adding \n",
    "#up the predictions of all the trees:\n",
    "\n",
    "X_new = np.array([[-0.4], [0.], [0.5]])\n",
    "sum(tree.predict(X_new) for tree in (tree_reg1, tree_reg2, tree_reg3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f0e829d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.57435458, 0.19765192, 0.19765192])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_reg1.predict(X_new) #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ba5cbd1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.04332508, -0.10506767,  0.42740054])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_reg2.predict(X_new) #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1b44258c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 7.25239814e-03, -8.12330666e-02,  4.27008856e-18])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_reg3.predict(X_new) #3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4d2cac32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-9 {color: black;background-color: white;}#sk-container-id-9 pre{padding: 0;}#sk-container-id-9 div.sk-toggleable {background-color: white;}#sk-container-id-9 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-9 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-9 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-9 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-9 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-9 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-9 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-9 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-9 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-9 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-9 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-9 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-9 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-9 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-9 div.sk-item {position: relative;z-index: 1;}#sk-container-id-9 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-9 div.sk-item::before, #sk-container-id-9 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-9 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-9 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-9 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-9 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-9 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-9 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-9 div.sk-label-container {text-align: center;}#sk-container-id-9 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-9 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-9\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GradientBoostingRegressor(learning_rate=1.0, max_depth=2, n_estimators=3,\n",
       "                          random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-16\" type=\"checkbox\" checked><label for=\"sk-estimator-id-16\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingRegressor</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingRegressor(learning_rate=1.0, max_depth=2, n_estimators=3,\n",
       "                          random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "GradientBoostingRegressor(learning_rate=1.0, max_depth=2, n_estimators=3,\n",
       "                          random_state=42)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Figure 7-9 represents the predictions of these three trees in the left column, and the ensemble’s\n",
    "#predictions in the right column. In the first row, the ensemble has just one tree, so its predictions\n",
    "#are exactly the same as the first tree’s predictions. In the second row, a new tree is trained on \n",
    "#the residual errors of the first tree. On the right you can see that the ensemble’s predictions are\n",
    "#equal to the sum of the predictions of the first two trees. Similarly, in the third row another \n",
    "#tree is trained on the residual errors of the second tree. You can see that the ensemble’s predictions\n",
    "#gradually get better as trees are added to the ensemble.\n",
    "#You can use Scikit-Learn’s GradientBoostingRegressor class to train GBRT ensembles more easily\n",
    "#(there’s also a GradientBoostingClassifier class for classification). Much like the RandomForestRegressor\n",
    "#class, it has hyperparameters to control the growth of decision trees (e.g., max_depth, min_samples_leaf),\n",
    "#as well as hyperparameters to control the ensemble training, such as the number of trees (n_estimators). \n",
    "#The following code creates the same ensemble as the previous one:\n",
    "\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=3, learning_rate=1.0,\n",
    "                                random_state=42)\n",
    "gbrt.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "44a86f8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-10 {color: black;background-color: white;}#sk-container-id-10 pre{padding: 0;}#sk-container-id-10 div.sk-toggleable {background-color: white;}#sk-container-id-10 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-10 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-10 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-10 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-10 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-10 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-10 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-10 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-10 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-10 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-10 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-10 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-10 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-10 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-10 div.sk-item {position: relative;z-index: 1;}#sk-container-id-10 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-10 div.sk-item::before, #sk-container-id-10 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-10 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-10 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-10 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-10 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-10 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-10 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-10 div.sk-label-container {text-align: center;}#sk-container-id-10 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-10 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-10\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GradientBoostingRegressor(learning_rate=0.05, max_depth=2, n_estimators=500,\n",
       "                          n_iter_no_change=10, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-17\" type=\"checkbox\" checked><label for=\"sk-estimator-id-17\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingRegressor</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingRegressor(learning_rate=0.05, max_depth=2, n_estimators=500,\n",
       "                          n_iter_no_change=10, random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "GradientBoostingRegressor(learning_rate=0.05, max_depth=2, n_estimators=500,\n",
       "                          n_iter_no_change=10, random_state=42)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The learning_rate hyperparameter scales the contribution of each tree. If you set it to a low value, \n",
    "#such as 0.05, you will need more trees in the ensemble to fit the training set, but the predictions \n",
    "#will usually generalize better. This is a regularization technique called shrinkage. Figure 7-10 shows\n",
    "#two GBRT ensembles trained with different hyperparameters: the one on the left does not have enough\n",
    "#trees to fit the training set, while the one on the right has about the right amount. If we added \n",
    "#more trees, the GBRT would start to overfit the training set.\n",
    "\n",
    "\n",
    "#To find the optimal number of trees, you could perform cross-validation using GridSearchCV or \n",
    "#RandomizedSearchCV, as usual, but there’s a simpler way: if you set the n_iter_no_change hyperparameter\n",
    "#to an integer value, say 10, then the GradientBoostingRegressor will automatically stop adding more\n",
    "#trees during training if it sees that the last 10 trees didn’t help. This is simply early stopping\n",
    "#(introduced in Chapter 4), but with a little bit of patience: it tolerates having no progress for a\n",
    "#few iterations before it stops. Let’s train the ensemble using early stopping:\n",
    "\n",
    "gbrt_best = GradientBoostingRegressor(\n",
    "            max_depth=2, learning_rate=0.05, \n",
    "            n_estimators=500, n_iter_no_change=10, random_state=42)\n",
    "gbrt_best.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d2587eda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#If you set n_iter_no_change too low, training may stop too early and the model will underfit. But if\n",
    "#you set it too high, it will overfit instead. We also set a fairly small learning rate and a high\n",
    "#number of estimators, but the actual number of estimators in the trained ensemble is much lower,\n",
    "#thanks to early stopping:\n",
    "\n",
    "gbrt_best.n_estimators_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c607727f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#When n_iter_no_change is set, the fit() method automatically splits the training set into a smaller\n",
    "#training set and a validation set: this allows it to evaluate the model’s performance each time it\n",
    "#adds a new tree. The size of the validation set is controlled by the validation_fraction hyperparameter,\n",
    "#which is 10% by default. The tol hyperparameter determines the maximum performance improvement that still\n",
    "#counts as negligible. It defaults to 0.0001.\n",
    "\n",
    "\n",
    "#The GradientBoostingRegressor class also supports a subsample hyperparameter, which specifies the\n",
    "#fraction of training instances to be used for training each tree. For example, if subsample=0.25,\n",
    "#then each tree is trained on 25% of the training instances, selected randomly. As you can probably\n",
    "#guess by now, this technique trades a higher bias for a lower variance. It also speeds up training\n",
    "#considerably. This is called stochastic gradient boosting.\n",
    "\n",
    "\n",
    "#Histogram-Based Gradient Boosting\n",
    "#Scikit-Learn also provides another GBRT implementation, optimized for large datasets: histogram-based\n",
    "#gradient boosting (HGB). It works by binning the input features, replacing them with integers. The number\n",
    "#of bins is controlled by the max_bins hyperparameter, which defaults to 255 and cannot be set any higher\n",
    "#than this. Binning can greatly reduce the number of possible thresholds that the training algorithm needs\n",
    "#to evaluate. Moreover, working with integers makes it possible to use faster and more memory-efficient\n",
    "#data structures. And the way the bins are built removes the need for sorting the features when training each tree.\n",
    "    \n",
    "\n",
    "#As a result, this implementation has a computational complexity of O(b×m) instead of O(n×m×log(m)), where b\n",
    "#is the number of bins, m is the number of training instances, and n is the number of features. In practice,\n",
    "#this means that HGB can train hundreds of times faster than regular GBRT on large datasets. However, binning\n",
    "#causes a precision loss, which acts as a regularizer: depending on the dataset, this may help reduce overfitting,\n",
    "#or it may cause underfitting.\n",
    "\n",
    "#Scikit-Learn provides two classes for HGB: HistGradientBoostingRegressor and HistGradientBoostingClassifier.\n",
    "#They’re similar to GradientBoostingRegressor and GradientBoostingClassifier, with a few notable differences:\n",
    "\n",
    "#Early stopping is automatically activated if the number of instances is greater than 10,000. You can \n",
    "#turn early stopping always on or always off by setting the early_stopping hyperparameter to True or False.\n",
    "#Subsampling is not supported. n_estimators is renamed to max_iter.\n",
    "#The only decision tree hyperparameters that can be tweaked are max_leaf_nodes, min_samples_leaf, and max_depth.\n",
    "#The HGB classes also have two nice features: they support both categorical features and missing values. \n",
    "#This simplifies preprocessing quite a bit. However, the categorical features must be represented as \n",
    "#integers ranging from 0 to a number lower than max_bins. You can use an OrdinalEncoder for this. \n",
    "#For example, here’s how to build and train a complete pipeline for the California housing dataset \n",
    "#introduced in Chapter 2:\n",
    "        \n",
    "        \n",
    "import pandas as pd\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "housing = pd.read_csv(\"/Users/rokosango/PhD/HandsOnML/handson-ml3/datasets/housing/housing.csv\")\n",
    "\n",
    "hgb_reg = make_pipeline(\n",
    "        make_column_transformer((OrdinalEncoder(), [\"ocean_proximity\"]),\n",
    "                               remainder=\"passthrough\"),\n",
    "    HistGradientBoostingRegressor(categorical_features=[0], random_state=42)\n",
    ")\n",
    "\n",
    "\n",
    "#The whole pipeline is just as short as the imports! No need for an imputer, scaler, or a one-hot encoder, \n",
    "#so it’s really convenient. Note that categorical_features must be set to the categorical column indices \n",
    "#(or a Boolean array). Without any hyperparameter tuning, this model yields an RMSE of about 47,600, which\n",
    "#is not too bad.\n",
    "\n",
    "#Several other optimized implementations of gradient boosting are available in the Python ML ecosystem:\n",
    "#in particular, XGBoost, CatBoost, and LightGBM. These libraries have been around for several years. \n",
    "#They are all specialized for gradient boosting, their APIs are very similar to Scikit- Learn’s, and they\n",
    "#provide many additional features, including GPU acceleration; you should definitely check them out!\n",
    "#Moreover, the TensorFlow Random Forests library provides optimized implementations of a variety of\n",
    "#random forest algorithms, including plain random forests, extra-trees, GBRT, and several more.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74409d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stacking\n",
    "#The last ensemble method we will discuss in this chapter is called stacking (short for stacked generalization).\n",
    "#18 It is based on a simple idea: instead of using trivial functions (such as hard voting) to aggregate the \n",
    "#predictions of all predictors in an ensemble, why don’t we train a model to perform this aggregation? Figure \n",
    "#7-11 shows such an ensemble performing a regression task on a new instance. Each of the bottom three predictors\n",
    "#predicts a different value (3.1, 2.7, and 2.9), and then the final predictor (called a blender, or a meta learner)\n",
    "#takes these predictions as inputs and makes the final prediction (3.0).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#If you evaluate this stacking model on the test set, you will find 92.8% accuracy, which is a bit better\n",
    "#than the voting classifier using soft voting, which got 92%.\n",
    "#In conclusion, ensemble methods are versatile, powerful, and fairly simple to use. Random forests, AdaBoost,\n",
    "#and GBRT are among the first models you should test for most machine learning tasks, and they particularly\n",
    "#shine with heterogeneous tabular data. Moreover, as they require very little preprocessing, they’re great\n",
    "#for getting a prototype up and running quickly. Lastly, ensemble methods like voting classifiers and stacking\n",
    "#classifiers can help push your system’s performance to its limits.\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52eafb6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401a3d48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c079b826",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b233ccb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cfdfbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be31d387",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6fdf2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b7f39d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e826c123",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
